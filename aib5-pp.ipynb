{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":31011,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import timm\nimport torch.nn.functional as F\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader\nfrom torchvision import datasets, transforms\nfrom torchvision.datasets import ImageFolder\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport os \nimport gdown\nfrom tqdm import tqdm\nimport copy\nfrom sklearn.metrics import confusion_matrix\nimport seaborn as sns","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-05-06T14:57:11.648583Z","iopub.execute_input":"2025-05-06T14:57:11.648847Z","iopub.status.idle":"2025-05-06T14:57:24.366170Z","shell.execute_reply.started":"2025-05-06T14:57:11.648806Z","shell.execute_reply":"2025-05-06T14:57:24.365596Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"url = \"https://drive.google.com/file/d/1HgJT06Y3QtlejY9cPKWYnCi9wfr0UHId/view?usp=sharing\"\noutput = \"/kaggle/working/dataset.zip\"\ngdown.download(url=url, output=output, fuzzy=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-06T14:57:27.341264Z","iopub.execute_input":"2025-05-06T14:57:27.342214Z","iopub.status.idle":"2025-05-06T14:57:36.549617Z","shell.execute_reply.started":"2025-05-06T14:57:27.342186Z","shell.execute_reply":"2025-05-06T14:57:36.548867Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from zipfile import ZipFile\n  \nwith ZipFile(\"/kaggle/working/dataset.zip\", 'r') as zObject:\n      zObject.extractall(\n        path='./')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-06T14:57:56.844978Z","iopub.execute_input":"2025-05-06T14:57:56.845564Z","iopub.status.idle":"2025-05-06T14:58:00.364797Z","shell.execute_reply.started":"2025-05-06T14:57:56.845542Z","shell.execute_reply":"2025-05-06T14:58:00.364257Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Preposesing Data","metadata":{}},{"cell_type":"code","source":"mean = [0.485, 0.456, 0.406]\nstd = [0.229, 0.224, 0.225]\n\ntrain_transform = transforms.Compose([\n    transforms.RandomResizedCrop(224, scale=(0.7, 1.0)), \n    transforms.RandomAffine(degrees=30, translate=(0.1, 0.1), shear=15),\n    transforms.RandomPerspective(distortion_scale=0.3, p=0.3),\n    transforms.RandomHorizontalFlip(p=0.5),\n    transforms.RandomVerticalFlip(p=0.3),\n    transforms.GaussianBlur(kernel_size=(5, 5), sigma=(0.1, 2.0)),\n    transforms.ToTensor(),\n    transforms.Normalize(mean, std)\n])\n\n# Transform สำหรับ validation\nval_transform = transforms.Compose([\n    transforms.Resize((224, 224)),\n    transforms.ToTensor(),\n    transforms.Normalize(mean, std)\n])\n\ntest_transform = transforms.Compose([\n    transforms.Resize((224, 224)),\n    transforms.ToTensor(),\n    transforms.Normalize(mean, std)\n])\n\ntrain_dir = '/kaggle/working/datasets/Images/Train'\nval_dir = '/kaggle/working/datasets/Images/Validation'\ntest_dir = '/kaggle/working/datasets/Images/Test'\n\ntrain_dataset = ImageFolder(root=train_dir, transform=train_transform)\nval_dataset = ImageFolder(root=val_dir, transform=val_transform)\ntest_dataset = ImageFolder(root=test_dir, transform=test_transform)\n\ntrain_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=4, pin_memory=True)\nval_loader = DataLoader(val_dataset, batch_size=32, shuffle=False, num_workers=4, pin_memory=True)\ntest_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, num_workers=4, pin_memory=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-06T14:58:11.683876Z","iopub.execute_input":"2025-05-06T14:58:11.684159Z","iopub.status.idle":"2025-05-06T14:58:11.706175Z","shell.execute_reply.started":"2025-05-06T14:58:11.684137Z","shell.execute_reply":"2025-05-06T14:58:11.705671Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torchvision\ndef denormalize(tensor, mean, std):\n    for t, m, s in zip(tensor, mean, std):\n        t.mul_(s).add_(m) \n    return tensor\ndata_iter = iter(train_loader)\nimages, labels = next(data_iter)\ndenorm_images = torch.stack([denormalize(img.clone(), mean, std) for img in images])\n\n# แปลงเป็น Grid และแสดงผล\nimg_grid = torchvision.utils.make_grid(denorm_images, nrow=8, padding=2, normalize=False)\nplt.figure(figsize=(12, 6))\nplt.imshow(np.transpose(img_grid.numpy(), (1, 2, 0))) \nplt.axis('off')\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-06T14:58:13.951743Z","iopub.execute_input":"2025-05-06T14:58:13.952443Z","iopub.status.idle":"2025-05-06T14:58:17.278892Z","shell.execute_reply.started":"2025-05-06T14:58:13.952419Z","shell.execute_reply":"2025-05-06T14:58:17.277939Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# CNN_MODEL","metadata":{}},{"cell_type":"code","source":"class SEBlock(nn.Module):\n    def __init__(self, channels, reduction=16):\n        super(SEBlock, self).__init__()\n        self.pool = nn.AdaptiveAvgPool2d(1)\n        self.fc = nn.Sequential(\n            nn.Linear(channels, channels // reduction),\n            nn.ReLU(),\n            nn.Linear(channels // reduction, channels),\n            nn.Sigmoid()\n        )\n\n    def forward(self, x):\n        b, c, _, _ = x.size()\n        y = self.pool(x).view(b, c)\n        y = self.fc(y).view(b, c, 1, 1)\n        return x * y.expand_as(x)\n\nclass CrossStageAttention(nn.Module):\n    def __init__(self, channels, num_heads=4):\n        super().__init__()\n        self.num_heads = num_heads\n        self.qkv = nn.Linear(channels, channels * 3)\n        self.proj = nn.Linear(channels, 64)\n        self.scale = (channels // num_heads) ** -0.5\n\n    def forward(self, x_list):\n        B = x_list[0].shape[0]\n        feats = [f.view(B, f.shape[1], -1).permute(0, 2, 1) for f in x_list]\n        x = torch.cat(feats, dim=1)\n\n        qkv = self.qkv(x).chunk(3, dim=-1)\n        q, k, v = map(lambda t: t.reshape(B, -1, t.shape[-1]).unsqueeze(1), qkv)\n        attn = (q @ k.transpose(-2, -1)) * self.scale\n        attn = attn.softmax(dim=-1)\n        x = (attn @ v).squeeze(1)\n        return self.proj(x.mean(dim=1))\n\nclass DynamicFeatureReducer(nn.Module):\n    def __init__(self, in_channels):\n        super().__init__()\n        self.conv = nn.Conv2d(in_channels, 128, kernel_size=1)\n        self.se = SEBlock(128)\n        self.pool = nn.AdaptiveAvgPool2d((7, 7))  # ลด spatial size\n\n    def forward(self, x):\n        x = self.se(self.conv(x))\n        return self.pool(x)\n\n# โมเดลหลัก \nclass CassavaDiseaseModel(nn.Module):\n    def __init__(self, num_classes):\n        super().__init__()\n        # 1. ใช้ ConvNeXtV2 Base (Pretrained)\n        self.backbone = timm.create_model('convnextv2_base.fcmae_ft_in22k_in1k', pretrained=True, features_only=True)\n        self.backbone.set_grad_checkpointing(True)  # ลด VRAM\n\n        # 2. Freeze ชั้นล่าง\n        for name, param in self.backbone.named_parameters():\n            if 'stages.3' in name or 'norm' in name:\n                param.requires_grad = True\n            else:\n                param.requires_grad = False\n\n        self.stage_indices = [2, 3] \n        self.feature_channels = [self.backbone.feature_info[i]['num_chs'] for i in self.stage_indices]\n\n        self.reducers = nn.ModuleList([\n            DynamicFeatureReducer(c) for c in self.feature_channels\n        ])\n        \n        self.cross_attention = CrossStageAttention(channels=128)\n\n        # Classifier\n        self.classifier = nn.Sequential(\n            nn.Linear(64, 128),\n            nn.GELU(),\n            nn.TransformerEncoderLayer(d_model=128, nhead=2, dim_feedforward=128),\n            nn.Linear(128, num_classes)\n        )\n\n        self.temperature = nn.Parameter(torch.ones(1))\n\n    def forward(self, x):\n        features = self.backbone(x)\n        reduced = [self.reducers[i](features[idx]) for i, idx in enumerate(self.stage_indices)]\n        x = self.cross_attention(reduced)\n        x = self.classifier(x)\n        return x / self.temperature\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-06T15:06:28.795487Z","iopub.execute_input":"2025-05-06T15:06:28.795747Z","iopub.status.idle":"2025-05-06T15:06:28.809585Z","shell.execute_reply.started":"2025-05-06T15:06:28.795729Z","shell.execute_reply":"2025-05-06T15:06:28.808942Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Loss Function","metadata":{}},{"cell_type":"code","source":"class LabelSmoothedCrossEntropy(nn.Module):\n    def __init__(self, smoothing=0.1):\n        super().__init__()\n        self.smoothing = smoothing\n\n    def forward(self, logits, target):\n        log_probs = logits.log_softmax(dim=-1)\n        nll = -log_probs.gather(dim=-1, index=target.unsqueeze(1)).squeeze(1)\n        smooth_loss = -log_probs.mean(dim=-1)\n        return (1.0 - self.smoothing) * nll + self.smoothing * smooth_loss\n\nclass FocalLoss(nn.Module):\n    def __init__(self, gamma=2, weight=None):\n        super().__init__()\n        self.gamma = gamma\n        self.weight = weight\n\n    def forward(self, inputs, targets):\n        ce_loss = nn.CrossEntropyLoss(weight=self.weight, reduction='none')(inputs, targets)\n        pt = torch.exp(-ce_loss)\n        focal_loss = ((1 - pt) ** self.gamma * ce_loss).mean()\n        return focal_loss\n\ncriterion = lambda x, y: 0.7 * LabelSmoothedCrossEntropy(smoothing=0.1)(x, y) + 0.3 * FocalLoss(gamma=2)(x, y)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-06T15:06:31.056021Z","iopub.execute_input":"2025-05-06T15:06:31.056640Z","iopub.status.idle":"2025-05-06T15:06:31.062652Z","shell.execute_reply.started":"2025-05-06T15:06:31.056616Z","shell.execute_reply":"2025-05-06T15:06:31.061993Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#  Train","metadata":{}},{"cell_type":"code","source":"def train_model(model, train_loader, val_loader, num_epochs, device, save_path='/kaggle/working/best_model.pth'):\n    criterion = nn.CrossEntropyLoss()\n    optimizer = optim.AdamW(model.parameters(), lr=3e-4, weight_decay=1e-5)\n    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=num_epochs)\n    scaler = GradScaler()\n\n    best_model_wts = copy.deepcopy(model.state_dict())\n    best_acc = 0.0\n\n    for epoch in range(num_epochs):\n        print(f\"\\nEpoch {epoch+1}/{num_epochs}\")\n        print(\"-\" * 30)\n\n        # Training\n        model.train()\n        running_loss = 0.0\n        correct = 0\n        total = 0\n\n        for images, labels in tqdm(train_loader, desc=\"Training\"):\n            images, labels = images.to(device), labels.to(device)\n\n            # ตรวจสอบ Label Shape\n            if len(labels.shape) > 1 and labels.shape[1] > 1:\n                _, labels = labels.max(dim=1)  # เปลี่ยน one-hot เป็น class index\n\n            optimizer.zero_grad()\n            with torch.amp.autocast('cuda'):  # ใช้รูปแบบใหม่\n                outputs = model(images)\n                loss = criterion(outputs, labels)\n\n            scaler.scale(loss).backward()\n            scaler.step(optimizer)\n            scaler.update()\n            optimizer.zero_grad()\n\n            _, preds = torch.max(outputs, 1)\n            running_loss += loss.item() * images.size(0)\n            correct += (preds == labels).sum().item()\n            total += labels.size(0)\n\n        train_loss = running_loss / total\n        train_acc = correct / total\n        print(f\"Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.4f}\")\n\n        # Validation\n        model.eval()\n        val_loss = 0.0\n        correct = 0\n        total = 0\n\n        with torch.no_grad():\n            for images, labels in tqdm(val_loader, desc=\"Validation\"):\n                images, labels = images.to(device), labels.to(device)\n\n                # ตรวจสอบ Label Shape\n                if len(labels.shape) > 1 and labels.shape[1] > 1:\n                    _, labels = labels.max(dim=1)\n\n                outputs = model(images)\n                loss = criterion(outputs, labels)\n\n                _, preds = torch.max(outputs, 1)\n                val_loss += loss.item() * images.size(0)\n                correct += (preds == labels).sum().item()\n                total += labels.size(0)\n\n        val_loss /= total\n        val_acc = correct / total\n        print(f\"Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.4f}\")\n\n        # Save best model\n        if val_acc > best_acc:\n            best_acc = val_acc\n            best_model_wts = copy.deepcopy(model.state_dict())\n            torch.save(model.state_dict(), save_path)\n            print(f\"✅ Saved Best Model: {save_path} (Val Acc: {best_acc:.4f})\")\n\n    model.load_state_dict(best_model_wts)\n    return model","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-06T15:33:23.769971Z","iopub.execute_input":"2025-05-06T15:33:23.770696Z","iopub.status.idle":"2025-05-06T15:33:23.781275Z","shell.execute_reply.started":"2025-05-06T15:33:23.770669Z","shell.execute_reply":"2025-05-06T15:33:23.780624Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel = CassavaDiseaseModel(num_classes=5).to(device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-06T15:33:26.562137Z","iopub.execute_input":"2025-05-06T15:33:26.562674Z","iopub.status.idle":"2025-05-06T15:33:28.246125Z","shell.execute_reply.started":"2025-05-06T15:33:26.562651Z","shell.execute_reply":"2025-05-06T15:33:28.245463Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"num_epochs = 100\nsave_path = '/kaggle/working/best_model.pth'\nmodel = train_model(model, train_loader, val_loader, num_epochs, device, save_path)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-06T15:33:34.752998Z","iopub.execute_input":"2025-05-06T15:33:34.753568Z","iopub.status.idle":"2025-05-06T17:29:36.360873Z","shell.execute_reply.started":"2025-05-06T15:33:34.753543Z","shell.execute_reply":"2025-05-06T17:29:36.359626Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"torch.save(model.state_dict(), '/kaggle/working/final_convnextv2_model.pth')\nprint(\"Final model saved to /kaggle/working/final_convnextv2_model.pth\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-06T17:29:48.421258Z","iopub.execute_input":"2025-05-06T17:29:48.421847Z","iopub.status.idle":"2025-05-06T17:29:48.890516Z","shell.execute_reply.started":"2025-05-06T17:29:48.421792Z","shell.execute_reply":"2025-05-06T17:29:48.889780Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# EVALUATE Confusion Metric and F1 Score ","metadata":{}},{"cell_type":"code","source":"# ฟังก์ชันประเมินโมเดล\ndef evaluate_model(model, test_loader):\n    model.eval()\n    test_preds = []\n    test_true = []\n    with torch.no_grad():\n        for images, labels in test_loader:\n            images, labels = images.to(device), labels.to(device)\n            outputs = model(images)\n            preds = torch.argmax(outputs, dim=1)\n            test_preds.extend(preds.cpu().numpy())\n            test_true.extend(labels.cpu().numpy())\n\n    test_f1 = f1_score(test_true, test_preds, average='macro')\n    print(f'Test F1 Score: {test_f1:.4f}')\n\n    # สร้าง Confusion Matrix\n    cm = confusion_matrix(test_true, test_preds)\n    plt.figure(figsize=(10, 8))\n    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=class_names, yticklabels=class_names)\n    plt.xlabel('Predicted')\n    plt.ylabel('True')\n    plt.title('Confusion Matrix')\n    plt.show()\n\n    return test_f1, cm","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-06T17:30:21.082703Z","iopub.execute_input":"2025-05-06T17:30:21.083326Z","iopub.status.idle":"2025-05-06T17:30:21.089494Z","shell.execute_reply.started":"2025-05-06T17:30:21.083297Z","shell.execute_reply":"2025-05-06T17:30:21.088753Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# โหลดโมเดลและประเมิน\nmodel.load_state_dict(torch.load(\"/kaggle/working/final_convnextv2_model.pth\"))\ntest_f1, cm = evaluate_model(model, test_loader)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-06T17:29:57.966878Z","iopub.execute_input":"2025-05-06T17:29:57.967173Z","iopub.status.idle":"2025-05-06T17:30:11.771833Z","shell.execute_reply.started":"2025-05-06T17:29:57.967153Z","shell.execute_reply":"2025-05-06T17:30:11.770737Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"with torch.no_grad():\n    for images, labels in test_loader:\n        images, labels = images.to(device), labels.to(device)\n        outputs = model(images)\n        _, predicted = torch.max(outputs, 1)\n\n        y_true.extend(labels.cpu().numpy())\n        y_pred.extend(predicted.cpu().numpy())\n\n# Accuracy\nacc = accuracy_score(y_true, y_pred)\nprint(f\"\\ nAccuracy: {acc:.f}\")\n\n# Classification report\nprint(\"\\n Classification Report:\")\nprint(classification_report(y_true, y_pred, target_names=test_dataset.classes, digits=4))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-06T17:31:24.395257Z","iopub.execute_input":"2025-05-06T17:31:24.395512Z","iopub.status.idle":"2025-05-06T17:31:27.828632Z","shell.execute_reply.started":"2025-05-06T17:31:24.395496Z","shell.execute_reply":"2025-05-06T17:31:27.827610Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"เปรียบเทียบกับ https://www.kaggle.com/code/pradiptadatta/cassava-leaf-disease-best-quality\n                                     precision    recall  f1-score   support\n\n     Cassava Bacterial Blight (CBB)       0.71      0.65      0.68       311\n     Cassava Brown Streak Disease (CBSD)  0.86      0.82      0.84       726\n     Cassava Green Mottle (CGM)           0.83      0.79      0.81       632\n     Cassava Mosaic Disease (CMD)         0.95      0.97      0.96      3163\n                            Healthy       0.76      0.78      0.77       579\n\n                           accuracy                           0.89      5411\n                          macro avg       0.82      0.80      0.81      5411\n                       weighted avg       0.89      0.89      0.89      5411","metadata":{}},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}